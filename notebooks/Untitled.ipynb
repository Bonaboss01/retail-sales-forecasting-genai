{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2789fae2-996d-4b62-ab07-e2c736e5c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee66ce9-5321-45aa-8cbd-3c448b6697f3",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot settings\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "raw = \"../data/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ed9cb39-cdb6-47e1-8c60-c5274fe97cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rt/0zxshr9s4g713_r6y5sjpqk80000gn/T/ipykernel_48090/529985598.py:2: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sales = pd.read_csv(f\"{DATA_DIR}/sunnybest_sales.csv\", parse_dates=[\"date\"])\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "sales = pd.read_csv(f\"{DATA_DIR}/sunnybest_sales.csv\", parse_dates=[\"date\"])\n",
    "products = pd.read_csv(f\"{DATA_DIR}/sunnybest_products.csv\")\n",
    "stores = pd.read_csv(f\"{DATA_DIR}/sunnybest_stores.csv\")\n",
    "calendar = pd.read_csv(f\"{DATA_DIR}/sunnybest_calendar.csv\", parse_dates=[\"date\"])\n",
    "promos = pd.read_csv(f\"{DATA_DIR}/sunnybest_promotions.csv\", parse_dates=[\"date\"])\n",
    "weather = pd.read_csv(f\"{DATA_DIR}/sunnybest_weather.csv\", parse_dates=[\"date\"])\n",
    "inventory = pd.read_csv(f\"{DATA_DIR}/sunnybest_inventory.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "\n",
    "# surpress warning\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=DtypeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25740fb0-37b3-4dbd-8e24-8ff9e804f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1227240, 43)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>price</th>\n",
       "      <th>regular_price</th>\n",
       "      <th>discount_pct</th>\n",
       "      <th>promo_flag</th>\n",
       "      <th>promo_type</th>\n",
       "      <th>revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_payday</th>\n",
       "      <th>season</th>\n",
       "      <th>temperature_c</th>\n",
       "      <th>rainfall_mm</th>\n",
       "      <th>weather_condition</th>\n",
       "      <th>promo_type_promo</th>\n",
       "      <th>discount_pct_promo</th>\n",
       "      <th>promo_flag_promo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>445838.0</td>\n",
       "      <td>445838</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Dry</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>2</td>\n",
       "      <td>500410.0</td>\n",
       "      <td>500410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000820.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Dry</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1003</td>\n",
       "      <td>2</td>\n",
       "      <td>399365.0</td>\n",
       "      <td>399365</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>798730.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Dry</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1004</td>\n",
       "      <td>4</td>\n",
       "      <td>305796.0</td>\n",
       "      <td>305796</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1223184.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Dry</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1005</td>\n",
       "      <td>5</td>\n",
       "      <td>462752.0</td>\n",
       "      <td>462752</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2313760.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Dry</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_id  product_id  units_sold     price  regular_price  \\\n",
       "0 2021-01-01         1        1001           0  445838.0         445838   \n",
       "1 2021-01-01         1        1002           2  500410.0         500410   \n",
       "2 2021-01-01         1        1003           2  399365.0         399365   \n",
       "3 2021-01-01         1        1004           4  305796.0         305796   \n",
       "4 2021-01-01         1        1005           5  462752.0         462752   \n",
       "\n",
       "   discount_pct  promo_flag promo_type    revenue  ...  is_weekend  \\\n",
       "0             0           0        NaN        0.0  ...       False   \n",
       "1             0           0        NaN  1000820.0  ...       False   \n",
       "2             0           0        NaN   798730.0  ...       False   \n",
       "3             0           0        NaN  1223184.0  ...       False   \n",
       "4             0           0        NaN  2313760.0  ...       False   \n",
       "\n",
       "   is_holiday  is_payday season temperature_c rainfall_mm weather_condition  \\\n",
       "0        True      False    Dry          30.6         3.7             Rainy   \n",
       "1        True      False    Dry          30.6         3.7             Rainy   \n",
       "2        True      False    Dry          30.6         3.7             Rainy   \n",
       "3        True      False    Dry          30.6         3.7             Rainy   \n",
       "4        True      False    Dry          30.6         3.7             Rainy   \n",
       "\n",
       "  promo_type_promo discount_pct_promo  promo_flag_promo  \n",
       "0              NaN                NaN               NaN  \n",
       "1              NaN                NaN               NaN  \n",
       "2              NaN                NaN               NaN  \n",
       "3              NaN                NaN               NaN  \n",
       "4              NaN                NaN               NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    sales\n",
    "    .merge(products, on=\"product_id\", how=\"left\", suffixes=(\"\", \"_product\"))\n",
    "    .merge(stores, on=\"store_id\", how=\"left\", suffixes=(\"\", \"_store\"))\n",
    "    .merge(calendar, on=\"date\", how=\"left\", suffixes=(\"\", \"_cal\"))\n",
    "    .merge(weather, on=[\"date\", \"city\"], how=\"left\", suffixes=(\"\", \"_weather\"))\n",
    "    .merge(promos, on=[\"date\", \"store_id\", \"product_id\"], how=\"left\", suffixes=(\"\", \"_promo\"))\n",
    ")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f930818e-b583-46ac-8147-7d81f2726169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'store_id', 'product_id', 'units_sold', 'price',\n",
       "       'regular_price', 'discount_pct', 'promo_flag', 'promo_type', 'revenue',\n",
       "       'starting_inventory', 'ending_inventory', 'stockout_occurred', 'city',\n",
       "       'store_size', 'category', 'product_name', 'category_product', 'brand',\n",
       "       'regular_price_product', 'cost_price', 'is_seasonal', 'warranty_months',\n",
       "       'store_name', 'city_store', 'area', 'region', 'store_type',\n",
       "       'store_size_store', 'year', 'month', 'day', 'day_of_week', 'is_weekend',\n",
       "       'is_holiday', 'is_payday', 'season', 'temperature_c', 'rainfall_mm',\n",
       "       'weather_condition', 'promo_type_promo', 'discount_pct_promo',\n",
       "       'promo_flag_promo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2a27be-f1ce-4e8f-b536-317351dc453d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Benin\n",
       "1          Benin\n",
       "2          Benin\n",
       "3          Benin\n",
       "4          Benin\n",
       "           ...  \n",
       "1227235     Ogwa\n",
       "1227236     Ogwa\n",
       "1227237     Ogwa\n",
       "1227238     Ogwa\n",
       "1227239     Ogwa\n",
       "Name: city, Length: 1227240, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a91b4f04-ed28-45af-9290-776dde667b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>temperature_c</th>\n",
       "      <th>rainfall_mm</th>\n",
       "      <th>weather_condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>Benin</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Rainy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>Benin</td>\n",
       "      <td>26.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>Benin</td>\n",
       "      <td>30.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Rainy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Benin</td>\n",
       "      <td>30.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>Benin</td>\n",
       "      <td>32.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Rainy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   city  temperature_c  rainfall_mm weather_condition\n",
       "0 2021-01-01  Benin           30.6          3.7             Rainy\n",
       "1 2021-01-02  Benin           26.1          0.0             Sunny\n",
       "2 2021-01-03  Benin           30.9          3.4             Rainy\n",
       "3 2021-01-04  Benin           30.3          1.1            Cloudy\n",
       "4 2021-01-05  Benin           32.4          3.5             Rainy"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b0adb8-f6c0-470f-9079-f6acaf1d001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv(f\"{DATA_DIR}/sunnybest_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ec2a528-d065-49c6-9fb2-8f8a82ba1652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>city</th>\n",
       "      <th>area</th>\n",
       "      <th>region</th>\n",
       "      <th>store_type</th>\n",
       "      <th>store_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SunnyBest Benin Main</td>\n",
       "      <td>Benin</td>\n",
       "      <td>Oredo</td>\n",
       "      <td>Edo South</td>\n",
       "      <td>Mall</td>\n",
       "      <td>Large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SunnyBest Ekpoma</td>\n",
       "      <td>Ekpoma</td>\n",
       "      <td>Esan West</td>\n",
       "      <td>Edo Central</td>\n",
       "      <td>High Street</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>SunnyBest Auchi</td>\n",
       "      <td>Auchi</td>\n",
       "      <td>Etsako West</td>\n",
       "      <td>Edo North</td>\n",
       "      <td>High Street</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SunnyBest Irrua</td>\n",
       "      <td>Irrua</td>\n",
       "      <td>Esan Central</td>\n",
       "      <td>Edo Central</td>\n",
       "      <td>Plaza</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SunnyBest Igueben</td>\n",
       "      <td>Igueben</td>\n",
       "      <td>Igueben</td>\n",
       "      <td>Edo Central</td>\n",
       "      <td>High Street</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_id            store_name     city          area       region  \\\n",
       "0         1  SunnyBest Benin Main    Benin         Oredo    Edo South   \n",
       "1         2      SunnyBest Ekpoma   Ekpoma     Esan West  Edo Central   \n",
       "2         3       SunnyBest Auchi    Auchi   Etsako West    Edo North   \n",
       "3         4       SunnyBest Irrua    Irrua  Esan Central  Edo Central   \n",
       "4         5     SunnyBest Igueben  Igueben       Igueben  Edo Central   \n",
       "\n",
       "    store_type store_size  \n",
       "0         Mall      Large  \n",
       "1  High Street     Medium  \n",
       "2  High Street     Medium  \n",
       "3        Plaza      Small  \n",
       "4  High Street      Small  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4bb5d-49c8-4082-a918-44ddcd9dd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0145c52-1c6f-4ea5-bd8d-db89f034a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed09ff-a432-4b79-8a3b-6f6e9d799ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df61c46f-808d-458c-9881-44a40d59146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93bb327-94a7-475c-9e3b-03a2e88b24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea2d4b-6e23-4204-8c26-410410453e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de93ee-5ac9-401f-9935-1c7265dd9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7fc7c-4c65-43f6-80eb-875bceb6839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.promo_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bae3bc-a0ab-4628-8f78-bad06499d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39822156-aca5-43b5-94cf-41e5ecfc7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b372cf-d7b1-48e0-a05d-982cceaf615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe9710-6f33-4e99-b67b-6c6e7493b294",
   "metadata": {},
   "source": [
    "## Stores Data Overview & Business Context\n",
    "\n",
    "The stores dataset provides **static reference information** about each physical retail location.  \n",
    "Each row represents **one store**, describing its geographical location and operational scale.  \n",
    "This data does not change daily and is used to **add contextual information** to sales records rather than to record sales events themselves.\n",
    "\n",
    "---\n",
    "\n",
    "### Store Data Grain\n",
    "\n",
    "- **One row = one store**\n",
    "- The dataset grows only when:\n",
    "  - a new store opens, or  \n",
    "  - store attributes are updated\n",
    "\n",
    "---\n",
    "\n",
    "### Key Store Columns & Meanings\n",
    "\n",
    "| Column | Business Meaning |\n",
    "|------|------------------|\n",
    "| store_id | Unique identifier for a physical retail store |\n",
    "| city | City in which the store is located |\n",
    "| store_size | Classification of store scale (e.g. Small, Medium, Large), reflecting capacity and expected footfall |\n",
    "\n",
    "---\n",
    "\n",
    "### How This Data Is Used\n",
    "\n",
    "Store attributes are **joined onto the sales data** to help explain differences in demand and performance across locations.  \n",
    "For example, larger stores or stores in certain cities may consistently sell higher volumes than others.\n",
    "\n",
    "---\n",
    "\n",
    "### Important Note\n",
    "\n",
    "The stores dataset **does not record sales**.  \n",
    "It provides **context** that helps interpret sales outcomes observed in the daily sales dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac140c-046c-4c53-92f5-acb645b7d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "products.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496906ae-3841-4d4e-a2ec-e4e40f4b7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34314ba0-84d7-4d65-ad90-cfb77b3c63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b7436-46b2-439f-b80e-3b367abaf484",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be629979-108c-47ef-83f6-2fea5750bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "promos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9211f-5d67-4048-9f4c-3b2f0f20481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b5127-9b2f-43eb-bda9-c28d056970fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2a711-439c-48db-a783-444cdf4eac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].min(), df['date'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762bb36c-38a7-47c0-923e-c0bb8d7745a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    sales\n",
    "    .merge(products, on=\"product_id\", how=\"left\", suffixes=(\"\", \"_product\"))\n",
    "    .merge(stores, on=\"store_id\", how=\"left\", suffixes=(\"\", \"_store\"))\n",
    "    .merge(calendar, on=\"date\", how=\"left\", suffixes=(\"\", \"_cal\"))\n",
    "    .merge(weather, on=[\"date\", \"city\"], how=\"left\", suffixes=(\"\", \"_weather\"))\n",
    "    .merge(promos, on=[\"date\", \"store_id\", \"product_id\"], how=\"left\", suffixes=(\"\", \"_promo\"))\n",
    ")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6eae0b-23ef-4e5e-bec5-0aa413a9ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'].min(), df['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b817d1-f6e0-4ca1-baef-9995e085cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sales columns:\", sales.columns.tolist())\n",
    "print(\"stores columns:\", stores.columns.tolist())\n",
    "\n",
    "df = sales.merge(products, on=\"product_id\", how=\"left\")\n",
    "df = df.merge(stores, on=\"store_id\", how=\"left\")\n",
    "\n",
    "print(\"df columns after stores merge:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504fef6-ae05-49f1-a71f-fbeea3f96de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"city_x\",\"city_y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95078e-2570-4236-aa1b-8890a283f1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80fa86-5c32-41d2-a29c-e0e18f7b4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6795d-56b2-4acb-9372-9cbd62cd05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores.city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13bc31aa-62ee-4469-94ec-6a2d736f8b12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make_dataset.py\n",
    "\n",
    "Builds the canonical merged dataset (df) for SunnyBest.\n",
    "\n",
    "- Loads raw CSVs from data/raw/\n",
    "- Deduplicates promos (1 row per date-store-product)\n",
    "- Merges sales + products + stores + calendar + weather (+ promos_event)\n",
    "- Writes merged output to data/processed/ (optional)\n",
    "\n",
    "This is the single source of truth for downstream notebooks, models, API, and dashboard.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _project_root(start: Optional[Path] = None) -> Path:\n",
    "    \"\"\"\n",
    "    Find project root by walking up until we see key repo files/folders.\n",
    "    This makes the code robust whether you run it from src/, notebooks/, or repo root.\n",
    "    \"\"\"\n",
    "    if start is None:\n",
    "        start = Path.cwd()\n",
    "\n",
    "    markers = {\"README.md\", \"data\", \"src\", \"requirements.txt\", \"pyproject.toml\"}\n",
    "    cur = start.resolve()\n",
    "\n",
    "    for _ in range(10):  # enough levels for typical repos\n",
    "        items = {p.name for p in cur.iterdir()} if cur.exists() else set()\n",
    "        if markers.intersection(items) >= {\"data\", \"src\"}:\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "\n",
    "    # fallback: current working directory\n",
    "    return start.resolve()\n",
    "\n",
    "\n",
    "def _read_csv(path: Path, parse_dates: Optional[list[str]] = None) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    return pd.read_csv(path, parse_dates=parse_dates, low_memory= False)\n",
    "\n",
    "\n",
    "def _standardise_date_col(df: pd.DataFrame, col: str = \"date\") -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main builder\n",
    "# -----------------------------\n",
    "\n",
    "    raw_dir: Optional[str | Path] = None,\n",
    "    processed_dir: Optional[str | Path] = None,\n",
    "    save: bool = True,\n",
    "    filename: str = \"sunnybest_merged_df.csv\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build and (optionally) save the merged dataset.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): merged dataset\n",
    "    \"\"\"\n",
    "\n",
    "    root = _project_root()\n",
    "    raw_path = Path(raw_dir) if raw_dir is not None else root / \"data\" / \"raw\"\n",
    "    proc_path = Path(processed_dir) if processed_dir is not None else root / \"data\" / \"processed\"\n",
    "\n",
    "# ---- Load raw datasets\n",
    "def hello (x):\n",
    "    sales = _read_csv(raw_path / \"sunnybest_sales.csv\", parse_dates=[\"date\"])\n",
    "    products = _read_csv(raw_path / \"sunnybest_products.csv\")\n",
    "    stores = _read_csv(raw_path / \"sunnybest_stores.csv\")\n",
    "    calendar = _read_csv(raw_path / \"sunnybest_calendar.csv\", parse_dates=[\"date\"])\n",
    "    weather = _read_csv(raw_path / \"sunnybest_weather.csv\", parse_dates=[\"date\"])\n",
    "    promos = _read_csv(raw_path / \"sunnybest_promotions.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "    # ---- Standardise date\n",
    "    sales = _standardise_date_col(sales)\n",
    "    calendar = _standardise_date_col(calendar)\n",
    "    weather = _standardise_date_col(weather)\n",
    "    promos = _standardise_date_col(promos)\n",
    "\n",
    "    # ---- Defensive cleaning / expected keys\n",
    "    required_sales_cols = {\"date\", \"store_id\", \"product_id\"}\n",
    "    missing = required_sales_cols - set(sales.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Sales is missing required columns: {missing}\")\n",
    "\n",
    "    # Ensure weather has city for merge\n",
    "    if \"city\" not in weather.columns:\n",
    "        raise ValueError(\"Weather dataset must contain 'city' column (keyed by date + city).\")\n",
    "\n",
    "    # Ensure stores has city (so df has city before weather merge)\n",
    "    if \"city\" not in stores.columns:\n",
    "        raise ValueError(\"Stores dataset must contain 'city' column (used to merge weather on date+city).\")\n",
    "\n",
    "    # ---- Deduplicate promotions to 1 row per date-store-product\n",
    "    if len(promos) > 0:\n",
    "        promos = (\n",
    "            promos.sort_values([\"date\", \"store_id\", \"product_id\"])\n",
    "                  .drop_duplicates(subset=[\"date\", \"store_id\", \"product_id\"], keep=\"last\")\n",
    "        )\n",
    "\n",
    "    # ---- Merge step-by-step (avoids suffix chaos)\n",
    "    df = sales.merge(products, on=\"product_id\", how=\"left\")\n",
    "    df = df.merge(stores, on=\"store_id\", how=\"left\")          # brings in 'city'\n",
    "    df = df.merge(calendar, on=\"date\", how=\"left\")\n",
    "\n",
    "    print(\"\\n--- DEBUG: columns after stores+calendar merge ---\")\n",
    "    print(\"sales has city?:\", \"city\" in sales.columns)\n",
    "    print(\"stores has city?:\", \"city\" in stores.columns)\n",
    "    print(\"df columns containing 'city':\", [c for c in df.columns if \"city\" in c.lower()])\n",
    "    print(\"sample df columns (first 30):\", df.columns.tolist()[:30])\n",
    "    print(\"sample df columns (last 30):\", df.columns.tolist()[-30:])\n",
    "    print(\"--- END DEBUG ---\\n\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "683bc044-07f7-41b7-b740-9f85bb5e9b49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mhello\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mhello\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhello\u001b[39m (x):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     sales = _read_csv(\u001b[43mraw_path\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33msunnybest_sales.csv\u001b[39m\u001b[33m\"\u001b[39m, parse_dates=[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     85\u001b[39m     products = _read_csv(raw_path / \u001b[33m\"\u001b[39m\u001b[33msunnybest_products.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m     stores = _read_csv(raw_path / \u001b[33m\"\u001b[39m\u001b[33msunnybest_stores.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'raw_path' is not defined"
     ]
    }
   ],
   "source": [
    "hello (\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9decb225-2ff2-45f9-b908-2ee9b7e05674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets\n",
      "Columns after stores merge:\n",
      "['city_x', 'city_y']\n",
      "⚠️ city not found — skipping weather merge\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (explicit, no magic)\n",
    "# -----------------------------\n",
    "# ROOT = Path(__file__).resolve().parents[2]   # repo root\n",
    "# RAW = ROOT / \"data\" / \"raw\"\n",
    "# PROCESSED = ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# print(\"Project root:\", ROOT)\n",
    "\n",
    "# -----------------------------\n",
    "# Load raw datasets\n",
    "# -----------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "RAW = Path(\"../data/raw\")\n",
    "\n",
    "sales = pd.read_csv(RAW / \"sunnybest_sales.csv\", parse_dates=[\"date\"], low_memory=False)\n",
    "products = pd.read_csv(RAW / \"sunnybest_products.csv\", low_memory=False)\n",
    "stores = pd.read_csv(RAW / \"sunnybest_stores.csv\", low_memory=False)\n",
    "calendar = pd.read_csv(RAW / \"sunnybest_calendar.csv\", parse_dates=[\"date\"], low_memory=False)\n",
    "weather = pd.read_csv(RAW / \"sunnybest_weather.csv\", parse_dates=[\"date\"], low_memory=False)\n",
    "promos = pd.read_csv(RAW / \"sunnybest_promotions.csv\", parse_dates=[\"date\"], low_memory=False)\n",
    "\n",
    "print(\"Loaded datasets\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Deduplicate promotions\n",
    "# -----------------------------\n",
    "if len(promos) > 0:\n",
    "    promos = (\n",
    "        promos.sort_values([\"date\", \"store_id\", \"product_id\"])\n",
    "              .drop_duplicates(subset=[\"date\", \"store_id\", \"product_id\"], keep=\"last\")\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Merge step by step\n",
    "# -----------------------------\n",
    "df = sales.merge(products, on=\"product_id\", how=\"left\")\n",
    "df = df.merge(stores, on=\"store_id\", how=\"left\")\n",
    "df = df.merge(calendar, on=\"date\", how=\"left\")\n",
    "\n",
    "print(\"Columns after stores merge:\")\n",
    "print([c for c in df.columns if \"city\" in c.lower()])\n",
    "\n",
    "# -----------------------------\n",
    "# Weather merge (TEMP: safe)\n",
    "# -----------------------------\n",
    "if \"city\" in df.columns:\n",
    "    df = df.merge(weather, on=[\"date\", \"city\"], how=\"left\")\n",
    "else:\n",
    "    print(\"⚠️ city not found — skipping weather merge\")\n",
    "\n",
    "# -----------------------------\n",
    "# Promotions merge\n",
    "# -----------------------------\n",
    "if len(promos) > 0:\n",
    "    promos = promos.rename(columns={\n",
    "        \"promo_type\": \"promo_type_event\",\n",
    "        \"discount_pct\": \"discount_pct_event\",\n",
    "        \"promo_flag\": \"promo_flag_event\",\n",
    "    })\n",
    "    df = df.merge(promos, on=[\"date\", \"store_id\", \"product_id\"], how=\"left\")\n",
    "else:\n",
    "    df[\"promo_type_event\"] = np.nan\n",
    "    df[\"discount_pct_event\"] = 0.0\n",
    "    df[\"promo_flag_event\"] = 0\n",
    "\n",
    "# -----------------------------\n",
    "# Clean promo fields\n",
    "# -----------------------------\n",
    "df[\"promo_flag_event\"] = df[\"promo_flag_event\"].fillna(0).astype(int)\n",
    "df[\"discount_pct_event\"] = df[\"discount_pct_event\"].fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# Save output\n",
    "# -----------------------------\n",
    "# PROCESSED.mkdir(exist_ok=True)\n",
    "# out_file = PROCESSED / \"sunnybest_merged_df.csv\"\n",
    "# df.to_csv(out_file, index=False)\n",
    "\n",
    "# print(\"✅ Saved merged dataset:\", out_file)\n",
    "# print(\"Final shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5643f401-21a3-412a-b6e1-90d48c845591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>temperature_c</th>\n",
       "      <th>rainfall_mm</th>\n",
       "      <th>weather_condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>Benin</td>\n",
       "      <td>30.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Rainy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>Benin</td>\n",
       "      <td>26.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>Benin</td>\n",
       "      <td>30.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Rainy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Benin</td>\n",
       "      <td>30.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>Benin</td>\n",
       "      <td>32.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Rainy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   city  temperature_c  rainfall_mm weather_condition\n",
       "0 2021-01-01  Benin           30.6          3.7             Rainy\n",
       "1 2021-01-02  Benin           26.1          0.0             Sunny\n",
       "2 2021-01-03  Benin           30.9          3.4             Rainy\n",
       "3 2021-01-04  Benin           30.3          1.1            Cloudy\n",
       "4 2021-01-05  Benin           32.4          3.5             Rainy"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b7fc382-bb12-42b1-b983-186e415ea365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>price</th>\n",
       "      <th>regular_price_x</th>\n",
       "      <th>discount_pct</th>\n",
       "      <th>promo_flag</th>\n",
       "      <th>promo_type</th>\n",
       "      <th>revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_payday</th>\n",
       "      <th>season</th>\n",
       "      <th>promo_type_event</th>\n",
       "      <th>discount_pct_event</th>\n",
       "      <th>promo_flag_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>445838.0</td>\n",
       "      <td>445838</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Dry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>2</td>\n",
       "      <td>500410.0</td>\n",
       "      <td>500410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000820.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Dry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1003</td>\n",
       "      <td>2</td>\n",
       "      <td>399365.0</td>\n",
       "      <td>399365</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>798730.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Dry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1004</td>\n",
       "      <td>4</td>\n",
       "      <td>305796.0</td>\n",
       "      <td>305796</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1223184.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Dry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1005</td>\n",
       "      <td>5</td>\n",
       "      <td>462752.0</td>\n",
       "      <td>462752</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2313760.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Dry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  store_id  product_id  units_sold     price  regular_price_x  \\\n",
       "0 2021-01-01         1        1001           0  445838.0           445838   \n",
       "1 2021-01-01         1        1002           2  500410.0           500410   \n",
       "2 2021-01-01         1        1003           2  399365.0           399365   \n",
       "3 2021-01-01         1        1004           4  305796.0           305796   \n",
       "4 2021-01-01         1        1005           5  462752.0           462752   \n",
       "\n",
       "   discount_pct  promo_flag promo_type    revenue  ...  month  day  \\\n",
       "0             0           0        NaN        0.0  ...      1    1   \n",
       "1             0           0        NaN  1000820.0  ...      1    1   \n",
       "2             0           0        NaN   798730.0  ...      1    1   \n",
       "3             0           0        NaN  1223184.0  ...      1    1   \n",
       "4             0           0        NaN  2313760.0  ...      1    1   \n",
       "\n",
       "   day_of_week is_weekend is_holiday is_payday season promo_type_event  \\\n",
       "0       Friday      False       True     False    Dry              NaN   \n",
       "1       Friday      False       True     False    Dry              NaN   \n",
       "2       Friday      False       True     False    Dry              NaN   \n",
       "3       Friday      False       True     False    Dry              NaN   \n",
       "4       Friday      False       True     False    Dry              NaN   \n",
       "\n",
       "  discount_pct_event  promo_flag_event  \n",
       "0                0.0                 0  \n",
       "1                0.0                 0  \n",
       "2                0.0                 0  \n",
       "3                0.0                 0  \n",
       "4                0.0                 0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d11861fd-d72a-4aa0-a896-96d6460042f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'store_id', 'product_id', 'units_sold', 'price',\n",
       "       'regular_price_x', 'discount_pct', 'promo_flag', 'promo_type',\n",
       "       'revenue', 'starting_inventory', 'ending_inventory',\n",
       "       'stockout_occurred', 'city_x', 'store_size_x', 'category_x',\n",
       "       'product_name', 'category_y', 'brand', 'regular_price_y', 'cost_price',\n",
       "       'is_seasonal', 'warranty_months', 'store_name', 'city_y', 'area',\n",
       "       'region', 'store_type', 'store_size_y', 'year', 'month', 'day',\n",
       "       'day_of_week', 'is_weekend', 'is_holiday', 'is_payday', 'season',\n",
       "       'promo_type_event', 'discount_pct_event', 'promo_flag_event'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e73ddeb-1459-4245-b9f2-bc3aed1361ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns for key 'store_id'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 194\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# CLI entry\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     _ = \u001b[43mbuild_merged_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 161\u001b[39m, in \u001b[36mbuild_merged_dataset\u001b[39m\u001b[34m(raw_dir, processed_dir, save, filename)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(promos) > \u001b[32m0\u001b[39m:\n\u001b[32m    156\u001b[39m     promos_renamed = promos.rename(columns={\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpromo_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpromo_type_event\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    158\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdiscount_pct\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdiscount_pct_event\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    159\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpromo_flag\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpromo_flag_event\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    160\u001b[39m     })\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpromos_renamed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    163\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mpromo_type_event\u001b[39m\u001b[33m\"\u001b[39m] = np.nan\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:10859\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10840\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10841\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10842\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10855\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10856\u001b[39m ) -> DataFrame:\n\u001b[32m  10857\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10860\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10865\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10868\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10869\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10873\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py:170\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[32m    156\u001b[39m         left_df,\n\u001b[32m    157\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         copy=copy,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     op = \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result(copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py:807\u001b[39m, in \u001b[36m_MergeOperation.__init__\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    803\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_tolerance(\u001b[38;5;28mself\u001b[39m.left_join_keys)\n\u001b[32m    805\u001b[39m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py:1509\u001b[39m, in \u001b[36m_MergeOperation._maybe_coerce_merge_keys\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1503\u001b[39m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[32m   1504\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1505\u001b[39m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[32m   1506\u001b[39m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1507\u001b[39m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[32m   1508\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1509\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1511\u001b[39m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[32m   1512\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk.dtype):\n",
      "\u001b[31mValueError\u001b[39m: You are trying to merge on object and int64 columns for key 'store_id'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "make_dataset.py\n",
    "\n",
    "Builds the canonical merged dataset (df) for SunnyBest.\n",
    "\n",
    "- Loads raw CSVs from data/raw/\n",
    "- Deduplicates promos (1 row per date-store-product)\n",
    "- Merges sales + products + stores + calendar + weather (+ promos_event)\n",
    "- Writes merged output to data/processed/ (optional)\n",
    "\n",
    "This is the single source of truth for downstream notebooks, models, API, and dashboard.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _project_root(start: Optional[Path] = None) -> Path:\n",
    "    \"\"\"\n",
    "    Find project root by walking up until we see key repo folders.\n",
    "    Robust whether run from src/, notebooks/, or repo root.\n",
    "    \"\"\"\n",
    "    if start is None:\n",
    "        start = Path.cwd()\n",
    "\n",
    "    cur = start.resolve()\n",
    "    for _ in range(10):\n",
    "        if (cur / \"data\").exists() and (cur / \"src\").exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "\n",
    "    return start.resolve()\n",
    "\n",
    "\n",
    "def _read_csv(path: Path, parse_dates: Optional[list[str]] = None) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    return pd.read_csv(path, parse_dates=parse_dates, low_memory=False)\n",
    "\n",
    "\n",
    "def _standardise_date_col(df: pd.DataFrame, col: str = \"date\") -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def _normalise_city(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize city strings for more reliable joins.\"\"\"\n",
    "    return (\n",
    "        series.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .replace({\"nan\": np.nan})\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main builder\n",
    "# -----------------------------\n",
    "def build_merged_dataset(\n",
    "    raw_dir: Optional[str | Path] = None,\n",
    "    processed_dir: Optional[str | Path] = None,\n",
    "    save: bool = True,\n",
    "    filename: str = \"sunnybest_merged_df.csv\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build and (optionally) save the merged dataset.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): merged dataset\n",
    "    \"\"\"\n",
    "    root = _project_root()\n",
    "    raw_path = Path(raw_dir) if raw_dir is not None else root / \"data\" / \"raw\"\n",
    "    proc_path = Path(processed_dir) if processed_dir is not None else root / \"data\" / \"processed\"\n",
    "\n",
    "    # ---- Load raw datasets\n",
    "    sales = _read_csv(raw_path / \"sunnybest_sales.csv\", parse_dates=[\"date\"])\n",
    "    products = _read_csv(raw_path / \"sunnybest_products.csv\")\n",
    "    stores = _read_csv(raw_path / \"sunnybest_stores.csv\")\n",
    "    calendar = _read_csv(raw_path / \"sunnybest_calendar.csv\", parse_dates=[\"date\"])\n",
    "    weather = _read_csv(raw_path / \"sunnybest_weather.csv\", parse_dates=[\"date\"])\n",
    "    promos = _read_csv(raw_path / \"sunnybest_promotions.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "    # ---- Standardise dates\n",
    "    sales = _standardise_date_col(sales)\n",
    "    calendar = _standardise_date_col(calendar)\n",
    "    weather = _standardise_date_col(weather)\n",
    "    promos = _standardise_date_col(promos)\n",
    "\n",
    "    # ---- Validate required keys\n",
    "    required_sales_cols = {\"date\", \"store_id\", \"product_id\"}\n",
    "    missing = required_sales_cols - set(sales.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Sales is missing required columns: {missing}\")\n",
    "\n",
    "    if \"city\" not in stores.columns:\n",
    "        raise ValueError(\"Stores dataset must contain 'city' column (used to merge weather on date+city).\")\n",
    "\n",
    "    if \"city\" not in weather.columns:\n",
    "        raise ValueError(\"Weather dataset must contain 'city' column (keyed by date + city).\")\n",
    "\n",
    "    # ---- Make store_id types consistent (prevents silent join issues)\n",
    "    # (Safe even if already same type)\n",
    "    sales[\"store_id\"] = sales[\"store_id\"].astype(str)\n",
    "    stores[\"store_id\"] = stores[\"store_id\"].astype(str)\n",
    "\n",
    "    # ---- Normalize city values in stores + weather to make join more reliable\n",
    "    stores[\"city\"] = _normalise_city(stores[\"city\"])\n",
    "    weather[\"city\"] = _normalise_city(weather[\"city\"])\n",
    "\n",
    "    # ---- Deduplicate promotions to 1 row per date-store-product\n",
    "    if len(promos) > 0:\n",
    "        promos = (\n",
    "            promos.sort_values([\"date\", \"store_id\", \"product_id\"])\n",
    "                  .drop_duplicates(subset=[\"date\", \"store_id\", \"product_id\"], keep=\"last\")\n",
    "        )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Merge step-by-step (clean + predictable)\n",
    "    # -----------------------------\n",
    "    df = sales.merge(products, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # Use suffixes to avoid city_x/city_y confusion\n",
    "    df = df.merge(stores, on=\"store_id\", how=\"left\", suffixes=(\"\", \"_store\"))\n",
    "\n",
    "    # Canonical city for weather MUST come from stores\n",
    "    if \"city_store\" in df.columns:\n",
    "        df[\"city\"] = df[\"city_store\"]\n",
    "    # If stores didn't create city_store (because df didn't have city already), city should already be present.\n",
    "    if \"city\" not in df.columns:\n",
    "        raise KeyError(\n",
    "            \"After merging stores, df still has no 'city' column. \"\n",
    "            \"Check stores['city'] and merge keys.\"\n",
    "        )\n",
    "\n",
    "    df = df.merge(calendar, on=\"date\", how=\"left\")\n",
    "\n",
    "    # ---- Debug (you can remove later)\n",
    "    # print(\"City-like columns:\", [c for c in df.columns if \"city\" in c.lower()])\n",
    "\n",
    "    # ---- Merge weather on date + city\n",
    "    df = df.merge(weather, on=[\"date\", \"city\"], how=\"left\")\n",
    "\n",
    "    # ---- Merge promotions (rename to avoid collisions)\n",
    "    if len(promos) > 0:\n",
    "        promos_renamed = promos.rename(columns={\n",
    "            \"promo_type\": \"promo_type_event\",\n",
    "            \"discount_pct\": \"discount_pct_event\",\n",
    "            \"promo_flag\": \"promo_flag_event\",\n",
    "        })\n",
    "        df = df.merge(promos_renamed, on=[\"date\", \"store_id\", \"product_id\"], how=\"left\")\n",
    "    else:\n",
    "        df[\"promo_type_event\"] = np.nan\n",
    "        df[\"discount_pct_event\"] = 0.0\n",
    "        df[\"promo_flag_event\"] = 0\n",
    "\n",
    "    # ---- Clean promo event fields\n",
    "    df[\"promo_flag_event\"] = df[\"promo_flag_event\"].fillna(0).astype(int)\n",
    "    df[\"discount_pct_event\"] = df[\"discount_pct_event\"].fillna(0)\n",
    "\n",
    "    # ---- Optional: warn if expected columns are missing\n",
    "    expected_cols = [\n",
    "        \"category\", \"season\", \"is_weekend\", \"is_holiday\", \"is_payday\",\n",
    "        \"temperature_c\", \"rainfall_mm\", \"weather_condition\"\n",
    "    ]\n",
    "    missing_expected = [c for c in expected_cols if c not in df.columns]\n",
    "    if missing_expected:\n",
    "        print(f\"⚠️ Note: merged df is missing expected columns: {missing_expected}\")\n",
    "\n",
    "    # ---- Save\n",
    "    if save:\n",
    "        proc_path.mkdir(parents=True, exist_ok=True)\n",
    "        out_file = proc_path / filename\n",
    "        df.to_csv(out_file, index=False)\n",
    "        print(f\"✅ Saved merged dataset: {out_file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI entry\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    _ = build_merged_dataset(save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "909c89d6-4cb2-4761-9944-3a579fafd158",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns for key 'store_id'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 194\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# CLI entry\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     _ = \u001b[43mbuild_merged_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 161\u001b[39m, in \u001b[36mbuild_merged_dataset\u001b[39m\u001b[34m(raw_dir, processed_dir, save, filename)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(promos) > \u001b[32m0\u001b[39m:\n\u001b[32m    156\u001b[39m     promos_renamed = promos.rename(columns={\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpromo_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpromo_type_event\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    158\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdiscount_pct\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdiscount_pct_event\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    159\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpromo_flag\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpromo_flag_event\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    160\u001b[39m     })\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpromos_renamed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    163\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mpromo_type_event\u001b[39m\u001b[33m\"\u001b[39m] = np.nan\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:10859\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10840\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10841\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10842\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10855\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10856\u001b[39m ) -> DataFrame:\n\u001b[32m  10857\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10860\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10865\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10868\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10869\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10873\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py:170\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[32m    156\u001b[39m         left_df,\n\u001b[32m    157\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         copy=copy,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     op = \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result(copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py:807\u001b[39m, in \u001b[36m_MergeOperation.__init__\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    803\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_tolerance(\u001b[38;5;28mself\u001b[39m.left_join_keys)\n\u001b[32m    805\u001b[39m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/reshape/merge.py:1509\u001b[39m, in \u001b[36m_MergeOperation._maybe_coerce_merge_keys\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1503\u001b[39m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[32m   1504\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1505\u001b[39m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[32m   1506\u001b[39m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1507\u001b[39m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[32m   1508\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1509\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1511\u001b[39m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[32m   1512\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk.dtype):\n",
      "\u001b[31mValueError\u001b[39m: You are trying to merge on object and int64 columns for key 'store_id'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "make_dataset.py\n",
    "\n",
    "Builds the canonical merged dataset (df) for SunnyBest.\n",
    "\n",
    "- Loads raw CSVs from data/raw/\n",
    "- Deduplicates promos (1 row per date-store-product)\n",
    "- Merges sales + products + stores + calendar + weather (+ promos_event)\n",
    "- Writes merged output to data/processed/ (optional)\n",
    "\n",
    "This is the single source of truth for downstream notebooks, models, API, and dashboard.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _project_root(start: Optional[Path] = None) -> Path:\n",
    "    \"\"\"\n",
    "    Find project root by walking up until we see key repo folders.\n",
    "    Robust whether run from src/, notebooks/, or repo root.\n",
    "    \"\"\"\n",
    "    if start is None:\n",
    "        start = Path.cwd()\n",
    "\n",
    "    cur = start.resolve()\n",
    "    for _ in range(10):\n",
    "        if (cur / \"data\").exists() and (cur / \"src\").exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "\n",
    "    return start.resolve()\n",
    "\n",
    "\n",
    "def _read_csv(path: Path, parse_dates: Optional[list[str]] = None) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    return pd.read_csv(path, parse_dates=parse_dates, low_memory=False)\n",
    "\n",
    "\n",
    "def _standardise_date_col(df: pd.DataFrame, col: str = \"date\") -> pd.DataFrame:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def _normalise_city(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize city strings for more reliable joins.\"\"\"\n",
    "    return (\n",
    "        series.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .replace({\"nan\": np.nan})\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main builder\n",
    "# -----------------------------\n",
    "def build_merged_dataset(\n",
    "    raw_dir: Optional[str | Path] = None,\n",
    "    processed_dir: Optional[str | Path] = None,\n",
    "    save: bool = True,\n",
    "    filename: str = \"sunnybest_merged_df.csv\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build and (optionally) save the merged dataset.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): merged dataset\n",
    "    \"\"\"\n",
    "    root = _project_root()\n",
    "    raw_path = Path(raw_dir) if raw_dir is not None else root / \"data\" / \"raw\"\n",
    "    proc_path = Path(processed_dir) if processed_dir is not None else root / \"data\" / \"processed\"\n",
    "\n",
    "    # ---- Load raw datasets\n",
    "    sales = _read_csv(raw_path / \"sunnybest_sales.csv\", parse_dates=[\"date\"])\n",
    "    products = _read_csv(raw_path / \"sunnybest_products.csv\")\n",
    "    stores = _read_csv(raw_path / \"sunnybest_stores.csv\")\n",
    "    calendar = _read_csv(raw_path / \"sunnybest_calendar.csv\", parse_dates=[\"date\"])\n",
    "    weather = _read_csv(raw_path / \"sunnybest_weather.csv\", parse_dates=[\"date\"])\n",
    "    promos = _read_csv(raw_path / \"sunnybest_promotions.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "    # ---- Standardise dates\n",
    "    sales = _standardise_date_col(sales)\n",
    "    calendar = _standardise_date_col(calendar)\n",
    "    weather = _standardise_date_col(weather)\n",
    "    promos = _standardise_date_col(promos)\n",
    "\n",
    "    # ---- Validate required keys\n",
    "    required_sales_cols = {\"date\", \"store_id\", \"product_id\"}\n",
    "    missing = required_sales_cols - set(sales.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Sales is missing required columns: {missing}\")\n",
    "\n",
    "    if \"city\" not in stores.columns:\n",
    "        raise ValueError(\"Stores dataset must contain 'city' column (used to merge weather on date+city).\")\n",
    "\n",
    "    if \"city\" not in weather.columns:\n",
    "        raise ValueError(\"Weather dataset must contain 'city' column (keyed by date + city).\")\n",
    "\n",
    "    # ---- Make store_id types consistent (prevents silent join issues)\n",
    "    # (Safe even if already same type)\n",
    "    sales[\"store_id\"] = sales[\"store_id\"].astype(str)\n",
    "    stores[\"store_id\"] = stores[\"store_id\"].astype(str)\n",
    "\n",
    "    # ---- Normalize city values in stores + weather to make join more reliable\n",
    "    stores[\"city\"] = _normalise_city(stores[\"city\"])\n",
    "    weather[\"city\"] = _normalise_city(weather[\"city\"])\n",
    "\n",
    "    # ---- Deduplicate promotions to 1 row per date-store-product\n",
    "    if len(promos) > 0:\n",
    "        promos = (\n",
    "            promos.sort_values([\"date\", \"store_id\", \"product_id\"])\n",
    "                  .drop_duplicates(subset=[\"date\", \"store_id\", \"product_id\"], keep=\"last\")\n",
    "        )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Merge step-by-step (clean + predictable)\n",
    "    # -----------------------------\n",
    "    df = sales.merge(products, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # Use suffixes to avoid city_x/city_y confusion\n",
    "    df = df.merge(stores, on=\"store_id\", how=\"left\", suffixes=(\"\", \"_store\"))\n",
    "\n",
    "    # Canonical city for weather MUST come from stores\n",
    "    if \"city_store\" in df.columns:\n",
    "        df[\"city\"] = df[\"city_store\"]\n",
    "    # If stores didn't create city_store (because df didn't have city already), city should already be present.\n",
    "    if \"city\" not in df.columns:\n",
    "        raise KeyError(\n",
    "            \"After merging stores, df still has no 'city' column. \"\n",
    "            \"Check stores['city'] and merge keys.\"\n",
    "        )\n",
    "\n",
    "    df = df.merge(calendar, on=\"date\", how=\"left\")\n",
    "\n",
    "    # ---- Debug (you can remove later)\n",
    "    # print(\"City-like columns:\", [c for c in df.columns if \"city\" in c.lower()])\n",
    "\n",
    "    # ---- Merge weather on date + city\n",
    "    df = df.merge(weather, on=[\"date\", \"city\"], how=\"left\")\n",
    "\n",
    "    # ---- Merge promotions (rename to avoid collisions)\n",
    "    if len(promos) > 0:\n",
    "        promos_renamed = promos.rename(columns={\n",
    "            \"promo_type\": \"promo_type_event\",\n",
    "            \"discount_pct\": \"discount_pct_event\",\n",
    "            \"promo_flag\": \"promo_flag_event\",\n",
    "        })\n",
    "        df = df.merge(promos_renamed, on=[\"date\", \"store_id\", \"product_id\"], how=\"left\")\n",
    "    else:\n",
    "        df[\"promo_type_event\"] = np.nan\n",
    "        df[\"discount_pct_event\"] = 0.0\n",
    "        df[\"promo_flag_event\"] = 0\n",
    "\n",
    "    # ---- Clean promo event fields\n",
    "    df[\"promo_flag_event\"] = df[\"promo_flag_event\"].fillna(0).astype(int)\n",
    "    df[\"discount_pct_event\"] = df[\"discount_pct_event\"].fillna(0)\n",
    "\n",
    "    # ---- Optional: warn if expected columns are missing\n",
    "    expected_cols = [\n",
    "        \"category\", \"season\", \"is_weekend\", \"is_holiday\", \"is_payday\",\n",
    "        \"temperature_c\", \"rainfall_mm\", \"weather_condition\"\n",
    "    ]\n",
    "    missing_expected = [c for c in expected_cols if c not in df.columns]\n",
    "    if missing_expected:\n",
    "        print(f\"⚠️ Note: merged df is missing expected columns: {missing_expected}\")\n",
    "\n",
    "    # ---- Save\n",
    "    if save:\n",
    "        proc_path.mkdir(parents=True, exist_ok=True)\n",
    "        out_file = proc_path / filename\n",
    "        df.to_csv(out_file, index=False)\n",
    "        print(f\"✅ Saved merged dataset: {out_file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI entry\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    _ = build_merged_dataset(save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069997eb-d583-4463-98a3-79ff85b86095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
