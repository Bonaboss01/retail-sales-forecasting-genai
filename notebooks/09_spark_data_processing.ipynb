{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3dcd628-0025-46d7-bb49-6424f8a0db83",
   "metadata": {},
   "source": [
    "# Spark Data Processing (Optional Scaling Layer)\n",
    "\n",
    "This notebook demonstrates an optional Spark-based ETL layer for the SunnyBest pipeline.\n",
    "The goal is to show how the project can scale beyond pandas when data volumes grow.\n",
    "\n",
    "> **Note on Spark:**  \n",
    "> This project does not strictly require Spark at its current scale. I included Spark as an optional processing layer to demonstrate how the pipeline could evolve in production as data volumes grow. The core modelling remains in pandas to support faster iteration during development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3dccc1-a071-4e5b-9cff-47db650a6120",
   "metadata": {},
   "source": [
    "## What this notebook covers\n",
    "\n",
    "- Create a local Spark session (or validate Spark availability)\n",
    "- Load SunnyBest raw CSVs using Spark\n",
    "- Run basic aggregations (revenue trends, category performance)\n",
    "- Write a feature-ready aggregated table to `data/processed/`\n",
    "- Outline how Spark output can feed a warehouse (e.g., Snowflake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7072f3d0-aaf3-48e2-b2f0-0d3ae1e2f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c9455d2-f2af-42cd-a3a1-818775633b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Spark availability check\n",
    "def spark_available():\n",
    "    try:\n",
    "        import pyspark  # noqa: F401\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "spark_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a37a27-b70b-4a40-a2aa-30bbb793184c",
   "metadata": {},
   "source": [
    "## Start a Spark session\n",
    "\n",
    "If `pyspark` is not installed locally, this notebook still documents the intended Spark workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0da6215-820c-4dcd-8b29-2e4c2a144032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark is not installed in this environment. Spark steps will be documented but not executed.\n"
     ]
    }
   ],
   "source": [
    "# create spark session(runs only if available)\n",
    "if spark_available():\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"SunnyBest-Spark-ETL\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark\n",
    "else:\n",
    "    print(\"pyspark is not installed in this environment. Spark steps will be documented but not executed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1b3b6-15d0-43b9-b34d-d4a0af818b73",
   "metadata": {},
   "source": [
    "## Load raw SunnyBest datasets\n",
    "\n",
    "We load the same raw CSVs used in pandas notebooks, but through Spark I/O.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ff9337-3e30-4692-be3c-32f36ec6655e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Spark CSV load (pyspark not available).\n"
     ]
    }
   ],
   "source": [
    "# load csvs\n",
    "RAW_DIR = \"../data/raw\"\n",
    "\n",
    "if spark_available():\n",
    "    sales_spark = spark.read.csv(f\"{RAW_DIR}/sunnybest_sales.csv\", header=True, inferSchema=True)\n",
    "    products_spark = spark.read.csv(f\"{RAW_DIR}/sunnybest_products.csv\", header=True, inferSchema=True)\n",
    "    stores_spark = spark.read.csv(f\"{RAW_DIR}/sunnybest_stores.csv\", header=True, inferSchema=True)\n",
    "    calendar_spark = spark.read.csv(f\"{RAW_DIR}/sunnybest_calendar.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    print(\"Sales rows:\", sales_spark.count())\n",
    "    sales_spark.printSchema()\n",
    "else:\n",
    "    print(\"Skipping Spark CSV load (pyspark not available).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39dbb31-de52-4f05-9fe6-2e20d2546c7e",
   "metadata": {},
   "source": [
    "## Join datasets (Spark version of the pandas merge)\n",
    "\n",
    "We join sales with product, store and calendar attributes to build an enriched table suitable for aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce44960-f3e8-4f5a-88ea-1eae0a4a5c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Spark joins (pyspark not available).\n"
     ]
    }
   ],
   "source": [
    "if spark_available():\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    df_spark = (\n",
    "        sales_spark\n",
    "        .join(products_spark, on=\"product_id\", how=\"left\")\n",
    "        .join(stores_spark, on=\"store_id\", how=\"left\")\n",
    "        .join(calendar_spark, on=\"date\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    print(\"Joined rows:\", df_spark.count())\n",
    "    df_spark.select(\"date\", \"store_id\", \"product_id\", \"category\", \"revenue\").show(5)\n",
    "else:\n",
    "    print(\"Skipping Spark joins (pyspark not available).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1aaef-0b50-41a3-bb6a-8ecd58072e69",
   "metadata": {},
   "source": [
    "## Aggregations for reporting\n",
    "\n",
    "Spark is often used upstream to generate:\n",
    "- daily revenue by store\n",
    "- daily revenue by category\n",
    "- promo-period performance tables\n",
    "These aggregated outputs can feed dashboards and warehouses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b742f051-1c24-439e-af91-eb654174dbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Spark aggregation (pyspark not available).\n"
     ]
    }
   ],
   "source": [
    "# Revenue by category (Spark)\n",
    "if spark_available():\n",
    "    from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "    revenue_by_category = (\n",
    "        df_spark\n",
    "        .groupBy(\"category\")\n",
    "        .agg(spark_sum(col(\"revenue\")).alias(\"total_revenue\"))\n",
    "        .orderBy(col(\"total_revenue\").desc())\n",
    "    )\n",
    "\n",
    "    revenue_by_category.show(20, truncate=False)\n",
    "else:\n",
    "    print(\"Skipping Spark aggregation (pyspark not available).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b265a9-ee1f-4e45-9b2a-01a3f3e2bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Spark daily aggregation (pyspark not available).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if spark_available():\n",
    "    from pyspark.sql.functions import to_date\n",
    "\n",
    "    daily_revenue = (\n",
    "        df_spark\n",
    "        .withColumn(\"date\", to_date(col(\"date\")))\n",
    "        .groupBy(\"date\")\n",
    "        .agg(spark_sum(col(\"revenue\")).alias(\"daily_revenue\"))\n",
    "        .orderBy(\"date\")\n",
    "    )\n",
    "\n",
    "    daily_revenue.show(10)\n",
    "else:\n",
    "    print(\"Skipping Spark daily aggregation (pyspark not available).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02687c33-f81b-488c-b1ea-7461c2bdfdee",
   "metadata": {},
   "source": [
    "## Write aggregated tables to data/processed/\n",
    "\n",
    "In a production pipeline, Spark writes curated tables to object storage or a warehouse.\n",
    "Here we write to `data/processed/` to keep the project self-contained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a40ba0-fe74-4f49-be4f-8e754ffa3988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping write (pyspark not available).\n"
     ]
    }
   ],
   "source": [
    "# write to processed(safe)\n",
    "PROCESSED_DIR = \"../data/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "if spark_available():\n",
    "    # Convert to pandas for simple CSV write in this project\n",
    "    revenue_cat_pd = revenue_by_category.toPandas()\n",
    "    revenue_cat_pd.to_csv(f\"{PROCESSED_DIR}/spark_revenue_by_category.csv\", index=False)\n",
    "\n",
    "    print(\"Saved:\", f\"{PROCESSED_DIR}/spark_revenue_by_category.csv\")\n",
    "else:\n",
    "    print(\"Skipping write (pyspark not available).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a03db3-2f45-4d39-856f-e45dfa245aae",
   "metadata": {},
   "source": [
    "## How Spark would connect to a warehouse (Snowflake)\n",
    "\n",
    "In a production environment, Spark outputs would be loaded into a warehouse such as Snowflake:\n",
    "\n",
    "1. **Raw tables** land in object storage (S3)\n",
    "2. Spark performs joins + feature engineering\n",
    "3. Curated outputs are written as **staging tables**\n",
    "4. Snowflake transforms staging tables into **analytics marts**\n",
    "5. Models and dashboards query marts for consistent business logic\n",
    "\n",
    "This project includes example SQL files under:\n",
    "`src/warehouse/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f269b8-66f2-4407-b6ec-7f7258f33111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
